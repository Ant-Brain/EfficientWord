{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1463fe",
   "metadata": {},
   "source": [
    "# Tflite Compatible log mel spectrogram calculator\n",
    "- Built to ensure cross platform uniform calculation of spectrogram \n",
    "- Can process only one waveform at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f9f578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:05:15.767811Z",
     "start_time": "2021-09-01T05:04:56.451204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.8.1-py3-none-any.whl (203 kB)\n",
      "\u001b[K     |████████████████████████████████| 203 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.14\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./siamese/lib/python3.9/site-packages (from librosa) (21.0)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "\u001b[K     |████████████████████████████████| 323 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0\n",
      "  Downloading scikit_learn-0.24.2-cp39-cp39-manylinux2010_x86_64.whl (23.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.8 MB 6.0 MB/s eta 0:00:01     |██████████████████████▌         | 16.8 MB 4.5 MB/s eta 0:00:02     |████████████████████████        | 17.8 MB 4.5 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in ./siamese/lib/python3.9/site-packages (from librosa) (5.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./siamese/lib/python3.9/site-packages (from librosa) (1.19.5)\n",
      "Collecting audioread>=2.0.0\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 998 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.43.0\n",
      "  Downloading numba-0.54.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.5.1-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.0.0\n",
      "  Downloading scipy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.5 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soundfile>=0.10.2\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in ./siamese/lib/python3.9/site-packages (from numba>=0.43.0->librosa) (57.0.0)\n",
      "Collecting llvmlite<0.38,>=0.37.0rc1\n",
      "  Downloading llvmlite-0.37.0-cp39-cp39-manylinux2014_x86_64.whl (26.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.3 MB 402 kB/s  eta 0:00:01     |████████████████████████        | 19.6 MB 10.1 MB/s eta 0:00:01     |██████████████████████████      | 21.4 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in ./siamese/lib/python3.9/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: requests in ./siamese/lib/python3.9/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Collecting appdirs\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: six>=1.3 in ./siamese/lib/python3.9/site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in ./siamese/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in ./siamese/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./siamese/lib/python3.9/site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./siamese/lib/python3.9/site-packages (from requests->pooch>=1.0->librosa) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./siamese/lib/python3.9/site-packages (from requests->pooch>=1.0->librosa) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./siamese/lib/python3.9/site-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
      "Building wheels for collected packages: audioread, resampy\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23153 sha256=6b6894f580b9948a4d2a4a3d5f520d320f7bac80511a254b8dae62aa1c2720b7\n",
      "  Stored in directory: /home/aman17/.cache/pip/wheels/a2/a3/bd/ec1568ce7515115a11ab686d509ad302124c782af065de47ee\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320732 sha256=072d93ae78ff1d095c75f093775181d41c4076720b3ba73e4053114e2fc1f40d\n",
      "  Stored in directory: /home/aman17/.cache/pip/wheels/86/2c/7d/46a32a246b0e5939cea2c5ec1492164073e0c5d16d666ae2cd\n",
      "Successfully built audioread resampy\n",
      "Installing collected packages: llvmlite, threadpoolctl, scipy, numba, joblib, appdirs, soundfile, scikit-learn, resampy, pooch, audioread, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.9 joblib-1.0.1 librosa-0.8.1 llvmlite-0.37.0 numba-0.54.0 pooch-1.5.1 resampy-0.2.2 scikit-learn-0.24.2 scipy-1.7.1 soundfile-0.10.3.post1 threadpoolctl-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:17:07.480544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-14 21:17:07.480575: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n",
    "import librosa\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f793a7df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:05:17.876716Z",
     "start_time": "2021-09-01T05:05:17.851517Z"
    }
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068c59f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:05:19.446495Z",
     "start_time": "2021-09-01T05:05:19.416530Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import fractions\n",
    "import datetime\n",
    "\n",
    "LOG_MEL_MEAN = 1.4\n",
    "LOG_MEL_STD = 1.184\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import fractions\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def _dft_matrix(dft_length):\n",
    "  \"\"\"Calculate the full DFT matrix in numpy.\"\"\"\n",
    "  omega = (0 + 1j) * 2.0 * np.pi / float(dft_length)\n",
    "  # Don't include 1/sqrt(N) scaling, tf.signal.rfft doesn't apply it.\n",
    "  return np.exp(omega * np.outer(np.arange(dft_length), np.arange(dft_length)))\n",
    "\n",
    "\n",
    "def _naive_rdft(signal_tensor, fft_length, padding='center'):\n",
    "    \"\"\"Implement real-input Fourier Transform by matmul.\"\"\"\n",
    "    # We are right-multiplying by the DFT matrix, and we are keeping\n",
    "    # only the first half (\"positive frequencies\").\n",
    "    # So discard the second half of rows, but transpose the array for\n",
    "    # right-multiplication.\n",
    "    # The DFT matrix is symmetric, so we could have done it more\n",
    "    # directly, but this reflects our intention better.\n",
    "    complex_dft_matrix_kept_values = _dft_matrix(fft_length)[:(fft_length // 2 + 1), :].transpose()\n",
    "    real_dft_tensor = tf.constant(np.real(complex_dft_matrix_kept_values).astype(np.float32), name='real_dft_matrix')\n",
    "    imag_dft_tensor = tf.constant(np.imag(complex_dft_matrix_kept_values).astype(np.float32), name='imaginary_dft_matrix')\n",
    "    signal_frame_length = signal_tensor.shape[-1]#.value\n",
    "    half_pad = (fft_length - signal_frame_length) // 2\n",
    "\n",
    "    if padding == 'center':\n",
    "        # Center-padding.\n",
    "        pad_values = tf.concat([\n",
    "            tf.zeros([tf.rank(signal_tensor) - 1, 2], tf.int32),\n",
    "            [[half_pad, fft_length - signal_frame_length - half_pad]]\n",
    "        ], axis=0)\n",
    "    elif padding == 'right':\n",
    "        # Right-padding.\n",
    "        pad_values = tf.concat([\n",
    "            tf.zeros([tf.rank(signal_tensor) - 1, 2], tf.int32),\n",
    "            [[0, fft_length - signal_frame_length]]\n",
    "        ], axis=0)\n",
    "\n",
    "    padded_signal = tf.pad(signal_tensor, pad_values)\n",
    "    \n",
    "    result_real_part = tf.matmul(padded_signal, real_dft_tensor)\n",
    "    result_imag_part = tf.matmul(padded_signal, imag_dft_tensor)\n",
    "    \n",
    "    return result_real_part, result_imag_part\n",
    "\n",
    "\n",
    "def _fixed_frame(signal, frame_length, frame_step, first_axis=False):\n",
    "    \"\"\"tflite-compatible tf.signal.frame for fixed-size input.\n",
    "    Args:\n",
    "        signal: Tensor containing signal(s).\n",
    "        frame_length: Number of samples to put in each frame.\n",
    "        frame_step: Sample advance between successive frames.\n",
    "        first_axis: If true, framing is applied to first axis of tensor; otherwise,\n",
    "        it is applied to last axis.\n",
    "    Returns:\n",
    "        A new tensor where the last axis (or first, if first_axis) of input\n",
    "        signal has been replaced by a (num_frames, frame_length) array of individual\n",
    "        frames where each frame is drawn frame_step samples after the previous one.\n",
    "    Raises:\n",
    "        ValueError: if signal has an undefined axis length.  This routine only\n",
    "        supports framing of signals whose shape is fixed at graph-build time.\n",
    "    \"\"\"\n",
    "    signal_shape = signal.shape.as_list()\n",
    "    \n",
    "    if first_axis:\n",
    "        length_samples = signal_shape[0]\n",
    "    else:\n",
    "        length_samples = signal_shape[-1]\n",
    "    \n",
    "    if length_samples <= 0:\n",
    "        raise ValueError('fixed framing requires predefined constant signal length')\n",
    "    \n",
    "    num_frames = max(0, 1 + (length_samples - frame_length) // frame_step)\n",
    "    \n",
    "    if first_axis:\n",
    "        inner_dimensions = signal_shape[1:]\n",
    "        result_shape = [num_frames, frame_length] + inner_dimensions\n",
    "        gather_axis = 0\n",
    "    else:\n",
    "        outer_dimensions = signal_shape[:-1]\n",
    "        result_shape = outer_dimensions + [num_frames, frame_length]\n",
    "        # Currently tflite's gather only supports axis==0, but that may still\n",
    "        # work if we want the last of 1 axes.\n",
    "        gather_axis = len(outer_dimensions)\n",
    "\n",
    "    subframe_length = math.gcd(frame_length, frame_step)  # pylint: disable=deprecated-method\n",
    "    subframes_per_frame = frame_length // subframe_length\n",
    "    subframes_per_hop = frame_step // subframe_length\n",
    "    num_subframes = length_samples // subframe_length\n",
    "\n",
    "    if first_axis:\n",
    "        trimmed_input_size = [num_subframes * subframe_length] + inner_dimensions\n",
    "        subframe_shape = [num_subframes, subframe_length] + inner_dimensions\n",
    "    else:\n",
    "        trimmed_input_size = outer_dimensions + [num_subframes * subframe_length]\n",
    "        subframe_shape = outer_dimensions + [num_subframes, subframe_length]\n",
    "    subframes = tf.reshape(\n",
    "        tf.slice(\n",
    "            signal,\n",
    "            begin=np.zeros(len(signal_shape), np.int32),\n",
    "            size=trimmed_input_size), subframe_shape)\n",
    "\n",
    "    # frame_selector is a [num_frames, subframes_per_frame] tensor\n",
    "    # that indexes into the appropriate frame in subframes. For example:\n",
    "    # [[0, 0, 0, 0], [2, 2, 2, 2], [4, 4, 4, 4]]\n",
    "    frame_selector = np.reshape(np.arange(num_frames) * subframes_per_hop, [num_frames, 1])\n",
    "\n",
    "    # subframe_selector is a [num_frames, subframes_per_frame] tensor\n",
    "    # that indexes into the appropriate subframe within a frame. For example:\n",
    "    # [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]\n",
    "    subframe_selector = np.reshape(np.arange(subframes_per_frame), [1, subframes_per_frame])\n",
    "\n",
    "    # Adding the 2 selector tensors together produces a [num_frames,\n",
    "    # subframes_per_frame] tensor of indices to use with tf.gather to select\n",
    "    # subframes from subframes. We then reshape the inner-most subframes_per_frame\n",
    "    # dimension to stitch the subframes together into frames. For example:\n",
    "    # [[0, 1, 2, 3], [2, 3, 4, 5], [4, 5, 6, 7]].\n",
    "    selector = frame_selector + subframe_selector\n",
    "    frames = tf.reshape(tf.gather(subframes, selector.astype(np.int32), axis=gather_axis), result_shape)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def _stft_tflite(signal, frame_length, frame_step, fft_length):\n",
    "    \"\"\"tflite-compatible implementation of tf.signal.stft.\n",
    "    Compute the short-time Fourier transform of a 1D input while avoiding tf ops\n",
    "    that are not currently supported in tflite (Rfft, Range, SplitV).\n",
    "    fft_length must be fixed. A Hann window is of frame_length is always\n",
    "    applied.\n",
    "    Since fixed (precomputed) framing must be used, signal.shape[-1] must be a\n",
    "    specific value (so \"?\"/None is not supported).\n",
    "    Args:\n",
    "        signal: 1D tensor containing the time-domain waveform to be transformed.\n",
    "        frame_length: int, the number of points in each Fourier frame.\n",
    "        frame_step: int, the number of samples to advance between successive frames.\n",
    "        fft_length: int, the size of the Fourier transform to apply.\n",
    "    Returns:\n",
    "        Two (num_frames, fft_length) tensors containing the real and imaginary parts\n",
    "        of the short-time Fourier transform of the input signal.\n",
    "    \"\"\"\n",
    "    # Make the window be shape (1, frame_length) instead of just frame_length\n",
    "    # in an effort to help the tflite broadcast logic.\n",
    "    window = tf.reshape(\n",
    "        tf.constant(\n",
    "            (0.5 - 0.5 * np.cos(2 * np.pi * np.arange(0, 1.0, 1.0 / frame_length))\n",
    "            ).astype(np.float32),\n",
    "            name='window'), [1, frame_length])\n",
    "    \n",
    "    framed_signal = _fixed_frame(signal, frame_length, frame_step, first_axis=False)\n",
    "    framed_signal *= window\n",
    "    \n",
    "    real_spectrogram, imag_spectrogram = _naive_rdft(framed_signal, fft_length)\n",
    "    \n",
    "    return real_spectrogram, imag_spectrogram\n",
    "\n",
    "\n",
    "def _stft_magnitude_tflite(signals, frame_length, frame_step, fft_length):\n",
    "    \"\"\"Calculate spectrogram avoiding tflite incompatible ops.\"\"\"\n",
    "    real_stft, imag_stft = _stft_tflite(signals, frame_length, frame_step, fft_length)\n",
    "    stft_magnitude = tf.sqrt(tf.add(real_stft * real_stft, imag_stft * imag_stft), name='magnitude_spectrogram')\n",
    "    \n",
    "    return stft_magnitude\n",
    "\n",
    "\n",
    "def waveform_to_log_mel_spectrogram(waveform):\n",
    "    \"\"\"Compute log mel spectrogram of a 1-D waveform.\"\"\"\n",
    "    with tf.name_scope('log_mel_features'):\n",
    "        # waveform has shape [<# samples>]\n",
    "\n",
    "        window_length_samples = 400 #int(round(params.SAMPLE_RATE * params.STFT_WINDOW_SECONDS))\n",
    "        hop_length_samples = 160    #int(round(params.SAMPLE_RATE * params.STFT_HOP_SECONDS))\n",
    "        MEL_BANDS   = 64            #params.MEL_BANDS\n",
    "        SAMPLE_RATE = 16000         #params.SAMPLE_RATE\n",
    "        LOG_OFFSET  = 0.001         #params.LOG_OFFSET\n",
    "        MEL_MIN_HZ  = 50            #params.MEL_MIN_HZ\n",
    "        MEL_MAX_HZ  = 8000         #params.MEL_MAX_HZ\n",
    "        fft_length  = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "        num_spectrogram_bins = fft_length // 2 + 1\n",
    "        magnitude_spectrogram = _stft_magnitude_tflite(\n",
    "            signals=waveform, \n",
    "            frame_length=window_length_samples, \n",
    "            frame_step=hop_length_samples, \n",
    "            fft_length=fft_length)\n",
    "\n",
    "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=MEL_BANDS,\n",
    "            num_spectrogram_bins=num_spectrogram_bins,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            lower_edge_hertz=MEL_MIN_HZ,\n",
    "            upper_edge_hertz=MEL_MAX_HZ)\n",
    "\n",
    "        mel_spectrogram = tf.matmul(\n",
    "            magnitude_spectrogram, linear_to_mel_weight_matrix)\n",
    "        log_mel_spectrogram = (tf.math.log(\n",
    "            mel_spectrogram + LOG_OFFSET) - LOG_MEL_MEAN)/LOG_MEL_STD\n",
    "        # log_mel_spectrogram has shape [<# STFT frames>, MEL_BANDS]\n",
    "\n",
    "        return log_mel_spectrogram\n",
    "\n",
    "def build_log_mel_graph():\n",
    "    inp = tf.keras.layers.Input(batch_shape=(None,16000))\n",
    "    return tf.keras.Model(\n",
    "        inputs=inp,\n",
    "        outputs=waveform_to_log_mel_spectrogram(\n",
    "            tf.squeeze(inp,axis=0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a05412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:05:22.292039Z",
     "start_time": "2021-09-01T05:05:21.355758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp39-cp39-manylinux1_x86_64.whl (10.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.3 MB 6.4 MB/s eta 0:00:01    |████▍                           | 1.4 MB 2.4 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in ./siamese/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./siamese/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16 in ./siamese/lib/python3.9/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: six in ./siamese/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.2 matplotlib-3.4.3 pillow-8.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import librosa\n",
    "import librosa.display # for waveplots, spectograms, etc\n",
    "import IPython.display as ipd # for playing files within pythonimport numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56158a89",
   "metadata": {},
   "source": [
    "#### Converting log mel spectrogram calculation to tflite for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce89dfc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:50:05.553666Z",
     "start_time": "2021-09-01T05:49:30.899713Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:21:14.597712: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm4_8yvtf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:21:17.982235: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-09-14 21:21:17.982405: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-14 21:21:17.984882: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.012ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-14 21:21:18.062476: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-14 21:21:18.062517: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-09-14 21:21:18.070170: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2021-09-14 21:21:18.099372: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1899] Estimated count of arithmetic ops: 54.938 M  ops, equivalently 27.469 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tflite optimized: 0.0026256754398345946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:21:20.763816: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No optimization: 0.048804286241531374\n"
     ]
    }
   ],
   "source": [
    "from time import sleep, time\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "logmelcalc = build_log_mel_graph()\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(logmelcalc)\n",
    "logmelcalc_tflite = converter.convert()\n",
    "\n",
    "logmelcalc_interpreter = tf.lite.Interpreter(model_content=logmelcalc_tflite)#allocate the tensors\n",
    "logmelcalc_interpreter.allocate_tensors()\n",
    "\n",
    "input_index = logmelcalc_interpreter.get_input_details()[0][\"index\"]\n",
    "output_details = logmelcalc_interpreter.get_output_details()\n",
    "\n",
    "s=time()\n",
    "for i in range(1000):\n",
    "    t = np.expand_dims(np.random.random(16000).astype(\"float32\"),axis=0)\n",
    "    logmelcalc_interpreter.set_tensor(input_index,t)\n",
    "    logmelcalc_interpreter.invoke()\n",
    "    output_data = logmelcalc_interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"tflite optimized:\",(time()-s)/1000)\n",
    "\n",
    "s = time()\n",
    "for i in range(1000):\n",
    "    q=logmelcalc.predict(t)\n",
    "print(\"No optimization:\",(time()-s)/1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f646a7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:52:42.061896Z",
     "start_time": "2021-09-01T05:51:01.614579Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dsnoop.c:638:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1075:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2660:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2660:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2660:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "ALSA lib pcm_oss.c:377:(_snd_pcm_oss_open) Unknown field port\n",
      "ALSA lib pcm_oss.c:377:(_snd_pcm_oss_open) Unknown field port\n",
      "ALSA lib pcm_a52.c:823:(_snd_pcm_a52_open) a52 is only for playback\n",
      "ALSA lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) Invalid type for card\n",
      "ALSA lib pcm_usb_stream.c:486:(_snd_pcm_usb_stream_open) Invalid type for card\n",
      "ALSA lib pcm_dmix.c:1075:(snd_pcm_dmix_open) unable to open slave\n",
      "Cannot connect to server socket err = No such file or directory\n",
      "Cannot connect to server request channel\n",
      "jack server is not running or cannot be started\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n",
      "JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.39566493034363\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "from time import time,sleep\n",
    "# import cv2\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "CHUNK = 4000\n",
    "RATE = 16000\n",
    "\n",
    "p=pyaudio.PyAudio()\n",
    "stream=p.open(format=pyaudio.paInt16,channels=1,rate=RATE,input=True,\n",
    "              frames_per_buffer=CHUNK)\n",
    "\n",
    "inpAudio = np.zeros(RATE)\n",
    "\n",
    "s = time()\n",
    "for i in range(int(100*RATE/CHUNK)): #go for a few seconds\n",
    "    #clear_output()\n",
    "    \n",
    "    data = np.frombuffer(stream.read(CHUNK),dtype=np.int16)\n",
    "    inpAudio = np.append(inpAudio[CHUNK:],data)\n",
    "\n",
    "    logmelcalc_interpreter.set_tensor(input_index,np.expand_dims(inpAudio/inpAudio.max(),axis=0).astype(\"float32\"))\n",
    "    logmelcalc_interpreter.invoke()\n",
    "    output_data = logmelcalc_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #plt.imshow(output_data)\n",
    "    #plt.show()\n",
    "    sleep(0.24)\n",
    "print(time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285ba2d",
   "metadata": {},
   "source": [
    "# Extracting BaseModel from Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ff32d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T06:14:53.560838Z",
     "start_time": "2021-09-01T06:14:51.406175Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad marshal data (unknown type code)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33418/1473857760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model-8-01-0.9551.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"basemodel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    198\u001b[0m         if (h5py is not None and\n\u001b[1;32m    199\u001b[0m             (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 200\u001b[0;31m           return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n\u001b[0m\u001b[1;32m    201\u001b[0m                                                   compile)\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    178\u001b[0m       \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    181\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    206\u001b[0m   \"\"\"\n\u001b[1;32m    207\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    209\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    675\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \"\"\"\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedObjectLoadingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m       input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0m\u001b[1;32m    663\u001b[0m           config, custom_objects)\n\u001b[1;32m    664\u001b[0m       model = cls(inputs=input_tensors, outputs=output_tensors,\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1271\u001b[0m   \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m     \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m   \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m   \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1253\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    206\u001b[0m   \"\"\"\n\u001b[1;32m    207\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    209\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    675\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \"\"\"\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedObjectLoadingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m       input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0m\u001b[1;32m    663\u001b[0m           config, custom_objects)\n\u001b[1;32m    664\u001b[0m       model = cls(inputs=input_tensors, outputs=output_tensors,\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1271\u001b[0m   \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m     \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m   \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m   \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1253\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m       \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    206\u001b[0m   \"\"\"\n\u001b[1;32m    207\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    209\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    675\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1003\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m     function = cls._parse_function_from_config(\n\u001b[0m\u001b[1;32m   1006\u001b[0m         config, custom_objects, 'function', 'module', 'function_type')\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m_parse_function_from_config\u001b[0;34m(cls, config, custom_objects, func_attr_name, module_attr_name, func_type_attr_name)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfunction_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m       \u001b[0;31m# Unsafe deserialization from bytecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m       function = generic_utils.func_load(\n\u001b[0m\u001b[1;32m   1058\u001b[0m           config[func_attr_name], globs=globs)\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfunction_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/wakeword_dataset_generator/siamese/lib/python3.9/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mfunc_load\u001b[0;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[1;32m    787\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mraw_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m   \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0mglobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad marshal data (unknown type code)"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.models.load_model(\"model-8-01-0.9551.h5\").get_layer(\"basemodel\")\n",
    "temp = tf.keras.Model(inputs=base_model.inputs,outputs=base_model.outputs)\n",
    "temp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a5357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T05:21:47.455418Z",
     "start_time": "2021-09-01T05:21:47.443666Z"
    }
   },
   "source": [
    "# Converting BaseModel to tflite for optimization\n",
    "- Not reducing number of floating points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a7504f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T09:22:33.448471Z",
     "start_time": "2021-09-01T09:22:20.424764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt0n4dsqm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpt0n4dsqm/assets\n",
      "/media/captain-america/7c8239eb-ca13-409c-8761-c865a3119962/Programming/UniversalHotwordDetector/.venv37/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-01 14:52:32.193567: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-09-01 14:52:32.193708: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-01 14:52:32.205759: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.01ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-01 14:52:32.822179: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-01 14:52:32.822207: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model speed\n",
      "0.005234899520874023\n",
      "(1, 128)\n"
     ]
    }
   ],
   "source": [
    "from time import sleep , time\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(source_model)\n",
    "base_model_tflite = converter.convert()\n",
    "\n",
    "base_model_interpreter = tf.lite.Interpreter(model_content=base_model_tflite)#allocate the tensors\n",
    "base_model_interpreter.allocate_tensors()\n",
    "\n",
    "base_model_inp = base_model_interpreter.get_input_details()\n",
    "base_model_out = base_model_interpreter.get_output_details()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Testing model speed\")\n",
    "\n",
    "temp = np.zeros((98,64))\n",
    "\n",
    "s = time()\n",
    "for i in range(100):\n",
    "    base_model_interpreter.set_tensor(\n",
    "        base_model_inp[0][\"index\"],\n",
    "        np.expand_dims(np.random.random((98,64)),axis=(0,-1)).astype(\"float32\")\n",
    "    )\n",
    "\n",
    "    base_model_interpreter.invoke()\n",
    "    output_data = base_model_interpreter.get_tensor(base_model_out[0][\"index\"])\n",
    "print((time()-s)/100)\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96915eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T08:39:06.601317Z",
     "start_time": "2021-09-01T08:39:06.555454Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 0,\n",
      "  'name': 'input_1',\n",
      "  'quantization': (0.0, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([], dtype=float32),\n",
      "                              'zero_points': array([], dtype=int32)},\n",
      "  'shape': array([ 1, 98, 64,  1], dtype=int32),\n",
      "  'shape_signature': array([-1, 98, 64,  1], dtype=int32),\n",
      "  'sparsity_parameters': {}}]\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 258,\n",
      "  'name': 'Identity',\n",
      "  'quantization': (0.0, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([], dtype=float32),\n",
      "                              'zero_points': array([], dtype=int32)},\n",
      "  'shape': array([  1, 128], dtype=int32),\n",
      "  'shape_signature': array([ -1, 128], dtype=int32),\n",
      "  'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "base_model_interpreter = tf.lite.Interpreter(model_content=base_model_tflite)#allocate the tensors\n",
    "base_model_interpreter.allocate_tensors()\n",
    "\n",
    "base_model_inp = base_model_interpreter.get_input_details()\n",
    "base_model_out = base_model_interpreter.get_output_details()\n",
    "\n",
    "pprint(base_model_inp)\n",
    "\n",
    "pprint(base_model_out)\n",
    "\n",
    "base_model_interpreter.set_tensor(\n",
    "        base_model_inp[0][\"index\"],\n",
    "        np.expand_dims(np.random.random((98,64)),axis=(0,-1)).astype(\"float32\")\n",
    "    )\n",
    "\n",
    "base_model_interpreter.invoke()\n",
    "output_data = base_model_interpreter.get_tensor(base_model_out[0][\"index\"])\n",
    "output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1d09ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T08:25:13.565586Z",
     "start_time": "2021-09-01T08:25:12.904779Z"
    }
   },
   "outputs": [],
   "source": [
    "t=base_model.predict(np.expand_dims(np.random.random((98,64)),axis=(0,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6732356e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T08:25:23.441619Z",
     "start_time": "2021-09-01T08:25:23.428593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcf62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
